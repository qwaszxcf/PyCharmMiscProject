"""
RAG é—®ç­”æ¨¡å—

åŠŸèƒ½ï¼š
1. å¯¹æ¥ç™¾ç‚¼å¹³å°å¤§æ¨¡å‹ï¼ˆqwen-plusï¼‰
2. ç»“åˆ FAISS æ£€ç´¢ç»“æœè¿›è¡Œé—®ç­”
3. å¼ºåˆ¶å¼•ç”¨æœºåˆ¶ï¼šæ¯ä¸ªç»“è®ºå¿…é¡»å¯¹åº” chunk_id
4. æ”¯æŒ JSON Schema ç»“æ„åŒ–è¾“å‡º
"""

import os
import json
from typing import List, Dict, Optional, Any
from dataclasses import dataclass

from openai import OpenAI
from faiss_indexer import FAISSIndexer


# ==================== é…ç½® ====================

@dataclass
class LLMConfig:
    """å¤§æ¨¡å‹é…ç½®"""
    model_name: str = "qwen-plus"  # å¯é€‰: qwen-turbo, qwen-max
    api_base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    temperature: float = 0.3  # é™ä½æ¸©åº¦ï¼Œä½¿å›ç­”æ›´ç²¾ç¡®
    max_tokens: int = 2048


@dataclass
class RAGConfig:
    """RAG æ£€ç´¢é…ç½®"""
    top_k: int = 5                    # æœ€å¤§æ£€ç´¢æ•°é‡
    min_chunks: int = 3               # æœ€å°‘ä½¿ç”¨ chunk æ•°
    max_chunks: int = 5               # æœ€å¤šä½¿ç”¨ chunk æ•°
    score_threshold: float = 0.35     # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼ä¸è¿›å…¥ prompt


# ==================== Prompt æ¨¡æ¿ ====================

RAG_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„é—®ç­”åŠ©æ‰‹ï¼Œå¿…é¡»ä¸¥æ ¼æ ¹æ®æä¾›çš„å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚

ã€æ ¸å¿ƒè§„åˆ™ - å¿…é¡»éµå®ˆã€‘
1. æ¯ä¸ªç»“è®ºå¿…é¡»è‡³å°‘å¼•ç”¨ä¸€ä¸ªèµ„æ–™ç¼–å·ï¼ˆå¦‚ [chunk_0001]ï¼‰
2. ç¦æ­¢å‡ºç°ä»»ä½•æœªå¼•ç”¨èµ„æ–™çš„åˆ¤æ–­æˆ–ç»“è®º
3. ç¦æ­¢ä½¿ç”¨"ç»¼åˆæ¥çœ‹"ã€"é€šå¸¸æƒ…å†µä¸‹"ã€"ä¸€èˆ¬è€Œè¨€"ç­‰æ³›åŒ–è¡¨è¿°
4. å¦‚æœèµ„æ–™ä¸è¶³ä»¥å›ç­”ï¼Œå¿…é¡»æ˜ç¡®è¯´æ˜"æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®š"
5. å›ç­”å¿…é¡»ç²¾å‡†ã€å…·ä½“ï¼Œç›´æ¥æŒ‡å‘èµ„æ–™å†…å®¹

ã€å›ç­”æ ¼å¼è¦æ±‚ã€‘
- ä½¿ç”¨ JSON æ ¼å¼è¾“å‡º
- å¿…é¡»åŒ…å« answerï¼ˆå›ç­”å†…å®¹ï¼‰å’Œ citationsï¼ˆå¼•ç”¨çš„èµ„æ–™IDåˆ—è¡¨ï¼‰
- citations ä¸­åªèƒ½åŒ…å«å®é™…å¼•ç”¨è¿‡çš„èµ„æ–™ID
- å›ç­”ä¸­çš„æ¯ä¸ªè¦ç‚¹åé¢å¿…é¡»æ ‡æ³¨ [èµ„æ–™ID]"""

RAG_USER_PROMPT_TEMPLATE = """ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

è¯·ä¸¥æ ¼æ ¹æ®ä¸Šè¿°å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚è¦æ±‚ï¼š
1. æ¯ä¸ªç»“è®ºåå¿…é¡»æ ‡æ³¨å¼•ç”¨æ¥æºï¼Œæ ¼å¼ï¼š[chunk_xxx]
2. åªä½¿ç”¨æä¾›çš„èµ„æ–™ï¼Œä¸è¦æ·»åŠ ä»»ä½•èµ„æ–™å¤–çš„ä¿¡æ¯
3. ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å« answer å’Œ citations å­—æ®µ

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
{{
    "answer": "æ ¹æ®è§„å®šï¼Œxxx [chunk_0001]ã€‚å…·ä½“è¦æ±‚æ˜¯ xxx [chunk_0002]ã€‚",
    "citations": ["chunk_0001", "chunk_0002"]
}}"""

NO_CONTEXT_RESPONSE = {
    "answer": "æŠ±æ­‰ï¼Œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ï¼Œæˆ–è€…æä¾›æ›´å¤šç»†èŠ‚ã€‚",
    "citations": []
}


# ==================== LLM å®¢æˆ·ç«¯ ====================

class LLMClient:
    """ç™¾ç‚¼å¹³å°å¤§æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰"""
    
    def __init__(self, config: LLMConfig = None):
        self.config = config or LLMConfig()
        
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            raise ValueError(
                "æœªæ‰¾åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡\n"
                "è¯·è®¾ç½®: $env:DASHSCOPE_API_KEY='your-api-key' (PowerShell)"
            )
        
        self.client = OpenAI(
            api_key=api_key,
            base_url=self.config.api_base_url
        )
    
    def chat(
        self, 
        messages: List[Dict[str, str]], 
        stream: bool = False
    ) -> str:
        """
        ä¸å¤§æ¨¡å‹å¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            æ¨¡å‹å›å¤æ–‡æœ¬
        """
        response = self.client.chat.completions.create(
            model=self.config.model_name,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            stream=stream
        )
        
        if stream:
            return self._handle_stream(response)
        
        return response.choices[0].message.content
    
    def _handle_stream(self, response) -> str:
        """å¤„ç†æµå¼å“åº”"""
        full_content = ""
        for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_content += content
        print()  # æ¢è¡Œ
        return full_content


# ==================== RAG é—®ç­”å¼•æ“ ====================

class RAGEngine:
    """RAG é—®ç­”å¼•æ“"""
    
    def __init__(
        self,
        indexer: FAISSIndexer = None,
        llm_config: LLMConfig = None,
        rag_config: RAGConfig = None
    ):
        """
        åˆå§‹åŒ– RAG å¼•æ“
        
        Args:
            indexer: FAISS ç´¢å¼•å™¨ï¼ˆå¦‚æœä¸º Noneï¼Œå°†è‡ªåŠ¨åŠ è½½ï¼‰
            llm_config: å¤§æ¨¡å‹é…ç½®
            rag_config: RAG æ£€ç´¢é…ç½®
        """
        # åˆå§‹åŒ–æˆ–åŠ è½½ç´¢å¼•å™¨
        if indexer is None:
            self.indexer = FAISSIndexer()
            self.indexer.load_index()
        else:
            self.indexer = indexer
        
        # åˆå§‹åŒ–é…ç½®
        self.llm_config = llm_config or LLMConfig()
        self.rag_config = rag_config or RAGConfig()
        
        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        self.llm = LLMClient(self.llm_config)
    
    def ask(
        self, 
        question: str, 
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        RAG é—®ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            {
                "answer": str,              # å›ç­”å†…å®¹ï¼ˆå¸¦å¼•ç”¨æ ‡æ³¨ï¼‰
                "citations": List[str],     # å¼•ç”¨çš„ chunk_id åˆ—è¡¨
                "sources": List[Dict],      # å®Œæ•´çš„å¼•ç”¨æ¥æºä¿¡æ¯
                "has_context": bool         # æ˜¯å¦æ‰¾åˆ°ç›¸å…³èµ„æ–™
            }
        """
        # 1. æ£€ç´¢ç›¸å…³ chunkï¼ˆåº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼‰
        print(f"ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³èµ„æ–™...")
        retrieved_chunks = self._retrieve_chunks(question)
        
        # 2. å¤„ç†æ— èµ„æ–™æƒ…å†µ
        if not retrieved_chunks:
            print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼ˆä½äºç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰")
            return {
                **NO_CONTEXT_RESPONSE,
                "sources": [],
                "has_context": False
            }
        
        print(f"âœ… æ‰¾åˆ° {len(retrieved_chunks)} ä¸ªç›¸å…³ç‰‡æ®µï¼ˆé˜ˆå€¼: {self.rag_config.score_threshold}ï¼‰")
        for chunk in retrieved_chunks:
            print(f"   - {chunk['chunk_id']} (score: {chunk['score']})")
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(retrieved_chunks)
        available_chunk_ids = [chunk["chunk_id"] for chunk in retrieved_chunks]
        
        # 4. æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": RAG_SYSTEM_PROMPT},
            {"role": "user", "content": RAG_USER_PROMPT_TEMPLATE.format(
                context=context,
                question=question
            )}
        ]
        
        # 5. è°ƒç”¨å¤§æ¨¡å‹
        print(f"\nğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...\n")
        raw_response = self.llm.chat(messages, stream=stream)
        
        # 6. è§£æå“åº”å¹¶éªŒè¯ citations
        result = self._parse_and_validate_response(
            raw_response, 
            available_chunk_ids,
            retrieved_chunks
        )
        
        return result
    
    def _retrieve_chunks(self, question: str) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³ chunkï¼Œåº”ç”¨é˜ˆå€¼å’Œæ•°é‡é™åˆ¶
        
        Returns:
            è¿‡æ»¤åçš„ chunk åˆ—è¡¨ï¼ˆ3-5ä¸ªï¼Œä¸”é«˜äºé˜ˆå€¼ï¼‰
        """
        # æ£€ç´¢ top_k ä¸ªç»“æœ
        results = self.indexer.search_with_chunks(
            query=question,
            top_k=self.rag_config.top_k,
            score_threshold=self.rag_config.score_threshold
        )
        
        # å·²ç»è¢« search_with_chunks è¿‡æ»¤æ‰ä½äºé˜ˆå€¼çš„ç»“æœ
        # é™åˆ¶æ•°é‡åœ¨ min_chunks ~ max_chunks ä¹‹é—´
        if len(results) > self.rag_config.max_chunks:
            results = results[:self.rag_config.max_chunks]
        
        return results
    
    def _build_context(self, chunks: List[Dict]) -> str:
        """å°† chunk åˆ—è¡¨æ„å»ºä¸ºä¸Šä¸‹æ–‡æ–‡æœ¬"""
        context_parts = []
        for chunk in chunks:
            chunk_id = chunk["chunk_id"]
            title = chunk.get("metadata", {}).get("title", "")
            text = chunk["text"]
            score = chunk["score"]
            
            # æ ¼å¼ï¼š[chunk_id] (ç›¸å…³åº¦: 0.xx) æ ‡é¢˜ï¼ˆå¦‚æœ‰ï¼‰\nå†…å®¹
            header = f"[{chunk_id}] (ç›¸å…³åº¦: {score})"
            if title:
                header += f" {title}"
            
            context_parts.append(f"{header}\n{text}")
        
        return "\n\n---\n\n".join(context_parts)
    
    def _parse_and_validate_response(
        self, 
        raw_response: str, 
        available_chunk_ids: List[str],
        retrieved_chunks: List[Dict]
    ) -> Dict[str, Any]:
        """
        è§£ææ¨¡å‹å“åº”å¹¶éªŒè¯ citations
        
        Args:
            raw_response: æ¨¡å‹åŸå§‹è¾“å‡º
            available_chunk_ids: å¯ç”¨çš„ chunk_id åˆ—è¡¨
            retrieved_chunks: æ£€ç´¢åˆ°çš„ chunk å®Œæ•´ä¿¡æ¯
        """
        # å°è¯•è§£æ JSON
        try:
            # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
            clean_response = raw_response.strip()
            if clean_response.startswith("```json"):
                clean_response = clean_response[7:]
            if clean_response.startswith("```"):
                clean_response = clean_response[3:]
            if clean_response.endswith("```"):
                clean_response = clean_response[:-3]
            clean_response = clean_response.strip()
            
            parsed = json.loads(clean_response)
            answer = parsed.get("answer", "")
            citations = parsed.get("citations", [])
            
        except json.JSONDecodeError:
            # JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬ä½œä¸ºç­”æ¡ˆ
            print("âš ï¸ JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å“åº”")
            answer = raw_response
            citations = []
        
        # éªŒè¯ citationsï¼šåªä¿ç•™å­˜åœ¨äº available_chunk_ids ä¸­çš„
        valid_citations = [
            cid for cid in citations 
            if cid in available_chunk_ids
        ]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆå¼•ç”¨
        invalid_citations = set(citations) - set(valid_citations)
        if invalid_citations:
            print(f"âš ï¸ ç§»é™¤äº†æ— æ•ˆå¼•ç”¨: {invalid_citations}")
        
        # æ„å»º sourcesï¼ˆå¼•ç”¨æ¥æºè¯¦æƒ…ï¼‰
        sources = []
        for chunk in retrieved_chunks:
            if chunk["chunk_id"] in valid_citations:
                sources.append({
                    "chunk_id": chunk["chunk_id"],
                    "score": chunk["score"],
                    "text": chunk["text"][:200] + "..." if len(chunk["text"]) > 200 else chunk["text"],
                    "metadata": chunk.get("metadata", {})
                })
        
        return {
            "answer": answer,
            "citations": valid_citations,
            "sources": sources,
            "has_context": True
        }


# ==================== äº¤äº’å¼é—®ç­” ====================

def interactive_qa():
    """äº¤äº’å¼é—®ç­” Demo"""
    print("=" * 60)
    print("RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼ˆå¸¦å¼ºåˆ¶å¼•ç”¨ï¼‰")
    print("=" * 60)
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("è¾“å…¥ 'config' æŸ¥çœ‹å½“å‰é…ç½®\n")
    
    # åˆå§‹åŒ– RAG å¼•æ“
    rag_config = RAGConfig(
        top_k=5,
        min_chunks=3,
        max_chunks=5,
        score_threshold=0.35
    )
    
    rag = RAGEngine(rag_config=rag_config)
    
    print(f"å½“å‰é…ç½®:")
    print(f"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {rag_config.score_threshold}")
    print(f"  - Chunk æ•°é‡: {rag_config.min_chunks}-{rag_config.max_chunks}")
    print(f"  - æ¨¡å‹: {rag.llm_config.model_name}")
    
    while True:
        try:
            question = input("\nğŸ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§!")
                break
            
            if question.lower() == 'config':
                print(f"\nå½“å‰é…ç½®:")
                print(f"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {rag_config.score_threshold}")
                print(f"  - Chunk æ•°é‡: {rag_config.min_chunks}-{rag_config.max_chunks}")
                print(f"  - æ¨¡å‹: {rag.llm_config.model_name}")
                continue
            
            print("-" * 40)
            result = rag.ask(question, stream=False)
            
            # æ˜¾ç¤ºç»“æœ
            print("\n" + "=" * 40)
            print("ğŸ“‹ å›ç­”:")
            print(result["answer"])
            
            print("\nğŸ“š å¼•ç”¨æ¥æº:")
            if result["citations"]:
                for cid in result["citations"]:
                    print(f"  - {cid}")
            else:
                print("  ï¼ˆæ— å¼•ç”¨ï¼‰")
            
            # æ˜¾ç¤ºæ¥æºè¯¦æƒ…
            if result["sources"]:
                print("\nğŸ“– æ¥æºè¯¦æƒ…:")
                for src in result["sources"]:
                    print(f"  [{src['chunk_id']}] (ç›¸å…³åº¦: {src['score']})")
                    print(f"    {src['text'][:100]}...")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    interactive_qa()
